{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oajEsh0WtSdW"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUlJZSOillTE"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Q3SkFEOLmD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, models, optimizers, initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_l_QCq3lwes"
      },
      "outputs": [],
      "source": [
        "download = drive.CreateFile({'id': '1X2HrMOY5q9KNOndpcMPjmAdW5M83p8Nb'})    # Pulling the dataset from Google Drive (Use the file id of the dataset to be used)\n",
        "download.GetContentFile('Dataset')          \n",
        "!7z x Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZq312uVm6ou"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0sOUctpt61-"
      },
      "outputs": [],
      "source": [
        "mv \"Split Dataset\" \"Split_Dataset\"\n",
        "builder = tfds.ImageFolder('./Split_Dataset')\n",
        "base_dir = \"./Split_Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjEDDeAeDOhg"
      },
      "outputs": [],
      "source": [
        "def fn(x):\n",
        "  x=(x-127.50)/127.50\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-U5gTv4nCRD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP0dsI7oUZz0",
        "outputId": "d3a60576-ea55-4e56-c6c5-58baac292d48"
      },
      "outputs": [],
      "source": [
        "train_gen_list = []\n",
        "n_of_classes = 10\n",
        "class_num_for_folder = []\n",
        "for i in range(17):\n",
        "  train_dir_cur = os.path.join(train_dir + str(i + 1))\n",
        "  train_gen_list.append(train_datagen.flow_from_directory(train_dir_cur, batch_size = 64, class_mode = 'sparse', color_mode = 'grayscale', target_size = (64, 64)))\n",
        "  if (i < 16):\n",
        "    class_num_for_folder.append(10)\n",
        "class_num_for_folder.append(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b24NRhuf9h8C"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "inp_size = 64\n",
        "inp_shape = (64, 64, 1)\n",
        "def get_generator(num_classes):   \n",
        "    input_noise = keras.layers.Input(shape = (110, ))\n",
        "    input_label = keras.layers.Input(shape = (1, ))\n",
        "    initial_dim_noise = 4 * 4 * 384\n",
        "    initial_dim_label = 16\n",
        "    initial_shape_noise = (4, 4, 384)\n",
        "    initial_shape_label = (4, 4, 1)\n",
        "    init = initializers.random_normal(stddev = 0.02)\n",
        "    x2 = layers.Embedding(num_classes, 50)(input_label)\n",
        "    x1 = layers.Dense(initial_dim_noise)(input_noise)\n",
        "    x1 = layers.Activation('relu')(x1)\n",
        "    x2 = layers.Dense(initial_dim_label)(x2)\n",
        "    x1 = layers.Reshape(initial_shape_noise)(x1)\n",
        "    x2 = layers.Reshape(initial_shape_label)(x2)\n",
        "    x = layers.Concatenate(axis = -1)([x1, x2])\n",
        "    x = layers.Conv2DTranspose(384, (5, 5), (2, 2), padding = 'same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2DTranspose(256, (5, 5), (2, 2), padding = 'same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2DTranspose(192, (5, 5), (2, 2), padding = 'same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2DTranspose(1, (5, 5), (2, 2), padding = 'same', kernel_initializer=init)(x)\n",
        "    x = layers.Activation('tanh')(x)\n",
        "    model = models.Model(inputs = [input_noise, input_label], outputs = x)\n",
        "    return model\n",
        "def get_discriminator(num_classes):\n",
        "    input = layers.Input(shape = inp_shape)\n",
        "    true_val = layers.Input(shape = (1,))\n",
        "    mid = layers.Embedding(num_classes, 50)(true_val)\n",
        "    mid = layers.Dense(512)(mid)\n",
        "    mid = layers.Dense(64*64)(mid)\n",
        "    mid = layers.Reshape(inp_shape)(mid);\n",
        "    x = layers.Concatenate(axis = -1)([input, mid])\n",
        "    init = initializers.RandomNormal(stddev = 0.02)\n",
        "    x = layers.Conv2D(16, (3, 3), (2, 2), padding = 'same', kernel_initializer = init)(input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Conv2D(32, (3, 3), (1, 1), padding = 'same', kernel_initializer = init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Conv2D(64, (3, 3), (2, 2), padding = 'same', kernel_initializer = init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Conv2D(128, (3, 3), (1, 1), padding = 'same', kernel_initializer = init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Conv2D(256, (3, 3), (2, 2), padding = 'same', kernel_initializer = init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Conv2D(512, (3, 3), (1, 1), padding = 'same', kernel_initializer = init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    y1 = layers.Dense(1, 'sigmoid')(x)\n",
        "    y2 = layers.Dense(num_classes, 'softmax')(x)\n",
        "    model = models.Model(inputs = [input, true_val], outputs = [y1, y2])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFmRoaB7ZVLm"
      },
      "outputs": [],
      "source": [
        "gen = get_generator(11)\n",
        "gen.summary()\n",
        "plot_model(gen, show_shapes = True, dpi = 64, rankdir='LR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_h7cTwWg1R6"
      },
      "outputs": [],
      "source": [
        "dis = get_discriminator(11)\n",
        "dis.summary()\n",
        "plot_model(dis, show_shapes = True, dpi = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtIfNu84vBk-",
        "outputId": "7c7ef24b-eb74-4102-ef90-9d8aa1690670"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hSJ6lhmg4iU"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "loss_object_binary=tensorflow.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "loss_object_categorical = tensorflow.keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hPJncJGkE4J"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(disc_real_class, input_label, disc_generated_class, disc_generated_output, disc_real_output):\n",
        "  \n",
        "  bloss=loss_object_binary(tf.ones_like(disc_real_output), disc_real_output)+loss_object_binary(tf.zeros_like(disc_generated_output),disc_generated_output)\n",
        "  closs=loss_object_categorical(input_label, disc_real_class)+loss_object_categorical(input_label, disc_generated_class)\n",
        "  return closs+bloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvKp4MkukNs_"
      },
      "outputs": [],
      "source": [
        "def generator_loss(disc_generated_output, disc_generated_class, input_label):\n",
        "  gan_loss = loss_object_binary(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  l1_loss = loss_object_categorical(input_label, disc_generated_class)\n",
        "  total_gen_loss = gan_loss + l1_loss\n",
        "  return total_gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgkYXlHCkQSv"
      },
      "outputs": [],
      "source": [
        "def train_step(generator, discriminator, input_image, input_label):\n",
        "  global current\n",
        "  BATCH_SIZE=len(input_image)\n",
        "  noise_dim=110\n",
        "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator([noise, input_label], training=True)\n",
        "\n",
        "    disc_real_output, disc_real_class = discriminator([input_image, input_label], training=True)\n",
        "    disc_generated_output, disc_generated_class = discriminator( [gen_output,input_label], training=True)\n",
        "\n",
        "    gen_total_loss = generator_loss(disc_generated_output, disc_generated_class, input_label)\n",
        "    disc_total_loss = discriminator_loss(disc_real_class, input_label, disc_generated_class, disc_generated_output, disc_real_output)\n",
        "    \n",
        "  \n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_total_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "  print(\"GenL\", gen_total_loss)\n",
        "  print(\"DisL\", disc_total_loss)\n",
        "  generator_optimizer = optimizers.Adam(0.0002, 0.5, 0.999)\n",
        "  discriminator_optimizer = optimizers.Adam(0.0002, 0.5, 0.999)\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thUGLkpFxxCw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def print_image(step, gen, model_num):\n",
        "  BATCH_SIZE=25\n",
        "  noise_dim=110\n",
        "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "  label = tf.random.uniform((BATCH_SIZE, ), 0, 9, dtype = tf.dtypes.int64)\n",
        "  checker = gen([noise, tf.dtypes.cast(label, tf.float32)], training=False)\n",
        "  image = np.array(checker)\n",
        "  image = (image + 1) / 2.0\n",
        "  for i in range(BATCH_SIZE):\n",
        "  # define subplot\n",
        "    plt.subplot(5, 5, 1 + i)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(image[i, :, :, 0], cmap='gray_r')\n",
        "  # save plot to file\n",
        "  filename1 = 'output_images/' + str(model_num) + '/generated_plot_%04d.png' % (step+1)\n",
        "  plt.savefig(filename1)\n",
        "  plt.close()\n",
        "def train_function(gen, disc, dataset, num_epochs, model_num):\n",
        "  count = len(dataset)\n",
        "  for j in range(num_epochs):\n",
        "    if (j % 100 == 0):\n",
        "      print(\"Current model number: \", model_num)\n",
        "    i = 0\n",
        "    print(\"Epoch number: \", j)\n",
        "    for i in  range(len(dataset)):\n",
        "      print(\"Batch: \", i)\n",
        "      batch = dataset[i]\n",
        "      image, label = batch\n",
        "      train_step(gen, disc, image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvpVm2_cnQIv"
      },
      "outputs": [],
      "source": [
        "gen_list = []\n",
        "disc_list = []\n",
        "for i in range(17):\n",
        "  gen_list.append(get_generator(class_num_for_folder[i]))\n",
        "  disc_list.append(get_discriminator(class_num_for_folder[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEbcMN5FLs4-"
      },
      "outputs": [],
      "source": [
        "def print_image_with_class(cname, gen):\n",
        "  BATCH_SIZE=25\n",
        "  noise_dim=110\n",
        "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "  label = tf.zeros([BATCH_SIZE]) + cname;\n",
        "  checker = gen([noise, tf.dtypes.cast(label, tf.float32)], training=False)\n",
        "  image = np.array(checker)\n",
        "  image = (image + 1) / 2.0\n",
        "  for i in range(BATCH_SIZE):\n",
        "  # define subplot\n",
        "    plt.subplot(5, 5, 1 + i)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(image[i, :, :, 0], cmap='gray_r')\n",
        "  # save plot to file\n",
        "  filename1 = 'output_images_with_class/generated_plot_%04d.png' % (cname)\n",
        "  plt.savefig(filename1)\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jyCK_3UNQMI"
      },
      "outputs": [],
      "source": [
        "def print_image_of_batch(dataset, idx):\n",
        "  image, labels=dataset[idx]\n",
        "  image = (image + 1) / 2.0\n",
        "  print(labels)\n",
        "  for i in range(20):\n",
        "  # define subplot\n",
        "    plt.subplot(5, 4, 1 + i)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(image[i, :, :, 0], cmap='gray_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nlh-wYZHHwCA",
        "outputId": "30530c94-5d26-4da8-ca15-d5b07d5643e6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "def save_checkpoint(gen, disc, model_num, epoch_num):\n",
        "  gen_path = \"ModelList/GenList/\"\n",
        "  disc_path = \"ModelList/DiscList/\"\n",
        "  gen_fin = gen_path + \"gen\" + str(model_num) + str(epoch_num)\n",
        "  disc_fin = disc_path + \"disc\" + str(model_num) + str(epoch_num)\n",
        "  gen.save(gen_fin)\n",
        "  disc.save(disc_fin)\n",
        "  !zip -r loss_rev_acgan.zip ModelList\n",
        "  !cp loss_rev_acgan.zip gdrive/MyDrive/\n",
        "\n",
        "\n",
        "for i in range(0,17):\n",
        "  train_function(gen_list[i], disc_list[i], train_gen_list[i], 100, i + 1)\n",
        "  save_checkpoint(gen_list[i], disc_list[i], i + 1, 100)\n",
        "  train_function(gen_list[i], disc_list[i], train_gen_list[i], 100, i + 1)\n",
        "  save_checkpoint(gen_list[i], disc_list[i], i + 1, 200)\n",
        "  train_function(gen_list[i], disc_list[i], train_gen_list[i], 100, i + 1)\n",
        "  save_checkpoint(gen_list[i], disc_list[i], i + 1, 300)\n",
        "  train_function(gen_list[i], disc_list[i], train_gen_list[i], 100, i + 1)\n",
        "  save_checkpoint(gen_list[i], disc_list[i], i + 1, 400)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plDz-bblS04E",
        "outputId": "54d40bbb-5b68-45fe-e610-6a0f8e82770e"
      },
      "outputs": [],
      "source": [
        "def get_checkpoint(model_num, epoch_num):\n",
        "  gen_path = \"ModelList/GenList/\"\n",
        "  disc_path = \"ModelList/DiscList/\"\n",
        "  gen_fin = gen_path + \"gen\" + str(model_num) + str(epoch_num)\n",
        "  disc_fin = disc_path + \"disc\" + str(model_num) + str(epoch_num)\n",
        "  gen = keras.models.load_model(gen_fin)\n",
        "  disc = keras.models.load_model(disc_fin)\n",
        "  return gen, disc\n",
        "def load_model_now():\n",
        "  !cp gdrive/MyDrive/random_ccgan.zip ./\n",
        "  !unzip random_ccgan.zip -d ./\n",
        "  for i in range(0,17):\n",
        "    try:\n",
        "      gen_list[i], disc_list[i] = get_checkpoint(i + 1, 100)\n",
        "      gen_list[i], disc_list[i] = get_checkpoint(i + 1, 200)\n",
        "      gen_list[i], disc_list[i] = get_checkpoint(i + 1, 400)\n",
        "    except:\n",
        "      pass\n",
        "load_model_now()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BengaliCCGANFinalSubmission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
